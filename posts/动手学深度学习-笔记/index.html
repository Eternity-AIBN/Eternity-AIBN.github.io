<!DOCTYPE html><html lang="en" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>《动手学深度学习》笔记 | Eternity-AIBN</title><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="《动手学深度学习》笔记" /><meta name="author" content="Augenstern" /><meta property="og:locale" content="en_US" /><meta name="description" content="一、MXNet 基础" /><meta property="og:description" content="一、MXNet 基础" /><link rel="canonical" href="https://eternity-aibn.github.io/posts/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/" /><meta property="og:url" content="https://eternity-aibn.github.io/posts/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/" /><meta property="og:site_name" content="Eternity-AIBN" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-08-26T00:00:00+08:00" /><meta name="google-site-verification" content="google-site-verification" /> <script type="application/ld+json"> {"description":"一、MXNet 基础","@type":"BlogPosting","headline":"《动手学深度学习》笔记","dateModified":"2020-08-26T00:00:00+08:00","datePublished":"2020-08-26T00:00:00+08:00","url":"https://eternity-aibn.github.io/posts/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/","mainEntityOfPage":{"@type":"WebPage","@id":"https://eternity-aibn.github.io/posts/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/"},"author":{"@type":"Person","name":"Augenstern"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/assets/js/post.min.js" async></script> <script src="/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/new.JPG" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Eternity-AIBN</a></div><div class="site-subtitle font-italic">一个很菜的菜鸡</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <!-- Switch the mode between dark and light. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <i class="mode-toggle fas fa-sun" dark-mode-invisible></i> <i class="mode-toggle fas fa-moon" light-mode-invisible></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightkMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightkMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href="https://github.com/Eternity-AIBN" target="_blank"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:window.open('mailto:' + ['181860044','smail.nju.edu.cn'].join('@'))" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" > <i class="fas fa-rss"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>《动手学深度学习》笔记</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Refactor the HTML structure. --> <!-- Suroundding the markdown table with '<div class="table-wrapper">. and '</div>' --> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>《动手学深度学习》笔记</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Aug 26, 2020, 12:00 AM +0800" > Aug 26, 2020 <i class="unloaded">2020-08-26T00:00:00+08:00</i> </span> by <span class="author"> Augenstern </span></div></div><div class="post-content"><h2 id="一mxnet-基础">一、MXNet 基础</h2><ol><li><p><code class="language-plaintext highlighter-rouge">asscalar</code>函数将向量转换为标量</p></li><li><p>广播机制</p></li><li><p>NDArray 与 Numpy 转换：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">D</span> <span class="o">=</span> <span class="n">nd</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>  <span class="c1"># Numpy-&gt;NDArray
</span><span class="n">D</span><span class="p">.</span><span class="n">asnumpy</span><span class="p">()</span> 	<span class="c1"># NDArray-&gt;Numpy
</span></pre></table></code></div></div></li><li><p>自动求梯度：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">x</span><span class="p">.</span><span class="n">attach_grad</span><span class="p">()</span>		<span class="c1"># 申请存储梯度所需要的内存
</span><span class="k">with</span> <span class="n">autograd</span><span class="p">.</span><span class="n">record</span><span class="p">():</span>	<span class="c1"># 要求MXNet记录与求梯度有关的计算
</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>		<span class="c1"># 自动求梯度
</span></pre></table></code></div></div></li><li><p>以线性回归模型为例：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre><span class="c1">#读取数据集
</span><span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">data</span> <span class="k">as</span> <span class="n">gdata</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">gdata</span><span class="p">.</span><span class="n">ArrayDataset</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">lables</span><span class="p">)</span> <span class="c1"># 将特征和标签组合
</span><span class="n">data_iter</span> <span class="o">=</span> <span class="n">gdata</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#随机读取小批量
</span>   
<span class="c1">#定义模型
</span><span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>  <span class="c1">#模型变量，串联各层的容器，构造模型时在该层依次添加层
</span><span class="n">net</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>   <span class="c1">#Dense实例——全连接层，无需指定输入形状，模型会自动推断
</span>   
<span class="c1">#初始化模型参数
</span><span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">init</span>
<span class="n">net</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">init</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">))</span> <span class="c1">#指定权重参数；偏差参数默认初始化为0
</span>   
<span class="c1">#定义损失函数
</span><span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">loss</span> <span class="k">as</span> <span class="n">gloss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">gloss</span><span class="p">.</span><span class="n">L2Loss</span><span class="p">()</span>  	<span class="c1">#平方损失（L2范数损失）
</span>   
<span class="c1">#定义优化算法
</span><span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">gluon</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">gluon</span><span class="p">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">collect_params</span><span class="p">(),</span><span class="s">'sgd'</span><span class="p">,{</span><span class="s">'learning_rate'</span><span class="p">:</span><span class="mf">0.03</span><span class="p">})</span>  	<span class="c1">#sgd小批量随机梯度下降
</span>   
<span class="c1">#训练模型
</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_ephchs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">autofrid</span><span class="p">.</span><span class="n">record</span><span class="p">():</span>
            <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>  	<span class="c1">#计算loss
</span>        <span class="n">l</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>			  <span class="c1">#反向传播
</span>        <span class="n">trainer</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> 	<span class="c1">#更新模型参数
</span>    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">feature</span><span class="p">),</span> <span class="n">lables</span><span class="p">)</span> 	<span class="c1">#计算全部loss
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'epoch %d, loss:%f'</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">l</span><span class="p">.</span><span class="n">mean</span><span class="p">().</span><span class="n">asnumpy</span><span class="p">()))</span>
</pre></table></code></div></div></li><li><p>计算图：可视化运算符和变量在计算中的依赖关系 方框代表变量，圆圈代表运算符，箭头表示从输入到输出之间的依赖关系</p></li><li><p>Xavier随机初始化：假设某全连接层输入个数为 a，输出个数为 b，该层中权重参数的每个元素<strong>随机采样</strong>于<strong>均匀分布</strong>：$U(-\sqrt \frac{6}{a+b},\sqrt \frac{6}{a+b})$ 主要考虑到：每层输出的方差不应该受该层输入个数影响，每层梯度的方差也不改受该层输出个数影响</p></li></ol><h2 id="二深度学习计算">二、深度学习计算</h2><h3 id="1-模型构造">1. 模型构造</h3><h4 id="1使用sequential类">1.使用Sequential类</h4><h4 id="2基于block类的构造方法">2.基于Block类的构造方法</h4><p>Block类是一个可供自由组建的部件，其子类既可以是<strong>一个层（Dense）</strong>，也可是<strong>一个模型</strong>，或者是<strong>模型的一部分</strong></p><p>eg. 继承Block类实现MLP：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">nd</span>
<span class="kn">from</span> <span class="nn">mxnet.gluon</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Block</span><span class="p">):</span>
    <span class="c1"># 声明带有模型参数的层，这里声明了两个全连接层
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 调用父类Block的构造函数进行必要的初始化，这样在构造实例时还可以指定其他函数
</span>        <span class="c1"># 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span>  <span class="c1"># 隐藏层
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># 输出层
</span>
    <span class="c1"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">output</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></table></code></div></div><p>（上述MLP类无需定义反向传播函数——系统将通过自动求梯度而自动生成backward函数）</p><h3 id="2-模型参数访问初始化共享">2. 模型参数访问、初始化、共享</h3><h4 id="1访问">1.访问</h4><p>对于Sequential构造的神经网络：</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="c1"># 访问第一层（[0]）包含的所有参数（params）
</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">params</span>

<span class="c1"># 访问特定参数 （Gluon中参数类型为Parameter类）
</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">params</span><span class="p">[</span><span class="s">'dense0_weight'</span><span class="p">]</span> <span class="c1">#通过名字访问字典里的元素
</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span>				   <span class="c1">#直接使用其变量名
</span>
<span class="c1"># 访问上诉Parameter类中的参数和梯度数值
</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">()</span>
<span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">grad</span><span class="p">()</span>

<span class="c1"># 获取net变量所有嵌套的层包含的参数
</span><span class="n">net</span><span class="p">.</span><span class="n">collect_params</span><span class="p">()</span>
<span class="c1"># 利用正则表达式匹配参数名
</span><span class="n">net</span><span class="p">.</span><span class="n">collect_params</span><span class="p">(</span><span class="s">'.*weight'</span><span class="p">)</span>
</pre></table></code></div></div><h4 id="2-初始化">2. 初始化</h4><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="c1"># 非首次对模型初始化需要指定force_reinit为真
# 正态分布数
</span><span class="n">net</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span> <span class="n">force_reinit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># 常数
</span><span class="n">net</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">force_reinit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># 对某个特定参数进行初始化——调用Parameter类的initialize函数
</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">.</span><span class="n">Xavier</span><span class="p">(),</span> <span class="n">force_reinit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># set_data直接改写模型参数
</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</pre></table></code></div></div><p>自定义初始化：实现一个<code class="language-plaintext highlighter-rouge">Initializer类</code>的子类，通常只需要实现<code class="language-plaintext highlighter-rouge">_init_weight</code>函数，将传入的NDArray修改成初始化的结果</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MyInit</span><span class="p">(</span><span class="n">init</span><span class="p">.</span><span class="n">Initializer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_init_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="p">...</span>
<span class="n">net</span><span class="p">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">MyInit</span><span class="p">(),</span> <span class="n">force_reinit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><h4 id="3-共享多个层之间">3. 共享（多个层之间）</h4><ol><li><p>重复调用同一个层</p></li><li><p>构造层时指定使用特定参数（使用同一份参数）</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)</span>
<span class="n">net</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">shared</span><span class="p">,</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">shared</span><span class="p">.</span><span class="n">params</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">net</span><span class="p">.</span><span class="n">initialize</span><span class="p">()</span>
</pre></table></code></div></div></li></ol><h3 id="3模型参数延后初始化">3.模型参数延后初始化</h3><ol><li>使用层时未指定输入个数——只有当将确定形状的输入<code class="language-plaintext highlighter-rouge">X</code>传进网络中进行前向计算<code class="language-plaintext highlighter-rouge">net(X)</code>时，系统才推断出该层的权重参数形状，此时才能真正开始初始化参数。</li><li>好处：模型更加简单，只需定义每个层的输出大小 坏处：第一次前向计算之前无法直接操作模型参数（如无法用data函数和set_data函数获取和修改参数）——通常会额外做一次前向计算迫使参数被真正初始化</li><li>如何避免：指定输入个数（使用<code class="language-plaintext highlighter-rouge">in_units</code>） 另，对已初始化的模型重新初始化时也不会发生延后初始化</li></ol><h3 id="4读取存储">4.读取、存储</h3><ol><li><p>直接使用<code class="language-plaintext highlighter-rouge">save</code>函数和<code class="language-plaintext highlighter-rouge">load</code>函数存储和读取NDArray</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">nd</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'x'</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>	<span class="c1">#将NDArray变量X存在名为x的文件中
</span><span class="n">x2</span><span class="o">=</span><span class="n">nd</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
</pre></table></code></div></div></li><li><p>读写Gluon模型参数：<code class="language-plaintext highlighter-rouge">save_parameters</code>和<code class="language-plaintext highlighter-rouge">load_parameter</code>函数</p></li></ol><h2 id="三卷积神经网络cnn">三、卷积神经网络CNN</h2><h3 id="1二维卷积层">1.二维卷积层</h3><ol><li>互相关运算</li><li>卷积层模型参数包括<strong>卷积核</strong>和<strong>标量偏差</strong>，通常先对卷积核随机初始化，然后不断迭代卷积核和偏差</li><li>特征图：二维卷积层输出的二维数组，可以看作输入在空间维度（宽和高）上某一级的表征 感受野：影响元素 x 的前向计算的所有可能输入区域</li></ol><h3 id="2填充与步幅">2.填充与步幅</h3><ol><li>步幅：输入元素无法填满窗口时<strong>无结果输出</strong> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824100823687.png" alt="image-20200824100823687" style="zoom:67%;" /></li><li>默认情况填充为 0 ，步幅为 1</li></ol><h3 id="3多输入输出通道">3.多输入/输出通道</h3><ol><li>第三维：通道维</li><li>输入数据含多个通道时，需要一个输入通道数与输入数据<strong>通道数相同</strong>的卷积核（<strong>逐通道计算，按通道相加</strong>）<img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824101154603.png" alt="image-20200824101154603" style="zoom: 67%;" /></li><li>由上，输出通道数总为1； 想得到多通道输出：每个输出通道使用一个核数组<img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824102721253.png" alt="image-20200824102721253" style="zoom: 50%;" /></li></ol><h3 id="4池化层">4.池化层</h3><ol><li><p>缓解卷积层对位置的过度敏感性</p></li><li><p>指定非正方形的池化窗口、填充和步幅</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></table></code></div></div></li><li><p>处理多通道数据时<strong>对每个通道分别池化</strong>——输出通道数与输入通道数相等</p></li></ol><h3 id="5卷积神经网络-lenet">5.卷积神经网络 LeNet</h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824165321810.png" alt="image-20200824165321810" /></p><ol><li>卷积层块 + 全连接层块</li><li>卷积层块：（卷积层+最大池化层）× 2 卷积层识别图像空间模式，最大池化层降低卷积层对位置敏感性 卷积层块输出形状：批量大小、通道、高、宽 全连接层会将小批量中每个样本变平（即输入形状变成二维）——批量大小、通道 * 高 * 宽</li></ol><h3 id="6深度卷积神经网络">6.深度卷积神经网络</h3><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824201439617.png" alt="image-20200824201439617" /></p><p>与LeNet区别： ① 包含<strong>8层变换</strong>——5层卷积和2层全连接隐藏层、1个全连接输出层 ② <strong>ReLU</strong>激活函数（更简单，消除<strong>梯度消失</strong>） ③ 使用 <strong>dropout</strong> 控制全连接层的模型复杂度 ④ 引入<strong>图像增广</strong>，扩大数据集以缓解过拟合</p><h3 id="7使用重复元素的网络-vgg">7.使用重复元素的网络 VGG</h3><ol><li>重复使用简单的基础块——连续使用数个相同的填充为1、窗口形状 3×3 的卷积层后接上一个步幅为2、窗口形状 2×2 的最大池化层</li><li>卷积层保持输入的高和宽不变，池化层对其减半</li><li>VGG-11：8个卷积层（单卷积层 * 2+双卷积层 * 3）+3个全连接层</li><li>高、宽减半，通道翻倍</li></ol><h3 id="8网络中的网络-nin">8.网络中的网络 NiN</h3><ol><li>上述三个网络：先卷积层抽取空间特征，再以全连接层输出分类结果 NiN：<strong>串联</strong>多个由卷积层和全连接层（实际上是 1×1 卷积层）构成的小网络</li><li>NiN块：一个卷积层 + 两个1×1卷积层（前者超参数自行设置，后者一般固定）</li><li>NiN模型：每个NiN块后接一个最大池化层 NiN 去掉了AlexNet最后的3个全连接层，使用<strong>输出通道数等于标签类别数的NiN块</strong>，然后用<strong>全局平均池化层</strong>对每个通道中所有元素<strong>求平均</strong>并直接用于分类——显著减小模型参数量、缓解过拟合 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200826200240356.png" alt="image-20200826200240356" style="zoom:50%;" /></li></ol><h3 id="9含并行连结的网络-googlenet">9.含并行连结的网络 GoogLeNet</h3><ol><li>Inception块 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200826204310913.png" alt="image-20200826204310913" /> 并行线路，都采用了合适的填充使输入输出高宽一致；可自定义的超参数为每个层中的<strong>输出通道数</strong></li><li><a href="https://static.oschina.net/uploads/space/2018/0317/141544_FfKB_876354.jpg">GoogLeNet模型</a>含 5 个模块，模块间使用最大池化层减小输出高宽 ① 64 通道的7×7卷积层 ② 1×1卷积 + 3×3卷积（对应第二条线路） ③ 串联2个完整的Inception块 ④ 串联5个Inception块 ⑤ 2个Inception块（后接全局平均化层、全连接层）</li></ol><h3 id="10批量归一化">10.批量归一化</h3><ol><li><p>标准化处理输入数据使<strong>各个特征的分布相近</strong>，<strong>易于训练出有效的模型</strong></p><p>训练中模型参数的更新容易造成<strong>靠近输出层</strong>输出的<strong>剧烈变化</strong>——数值的<strong>不稳定性</strong>，导致难以训练出有效模型</p><p>批量归一化<strong>不断调整神经网络中间输出</strong></p></li><li><p>对全连接层：置于仿射变化和激活函数之间 先对小批量求均值$μ_B$ 和方差$\sigma <em>B^2$ ，然后标准化（按元素开方和除法）$x^{(i)}</em>{hat}=\frac{x^{(i)}-μ_B}{\sqrt{\sigma^2<em>B+\epsilon}}$ 引入拉伸参数$\gamma$和偏移参数$\beta$，按元素乘法：$y^{(i)}=\gamma ⊙x^{(i)}</em>{hat}+\beta$</p></li><li><p>对卷积层：卷积计算后、激活函数前。含多通道时各通道单独进行批量归一化</p></li><li><p>训练时可将批量设置<strong>大一点</strong>，使均值和方差计算较为准确； 预测时使用<strong>移动平均</strong>估算均值和方差.</p></li></ol><h3 id="11残差网络-resnet">11.残差网络 ResNet</h3><ol><li>残差块：输入可通过跨层的数据线路更快向前传播</li></ol></div><div class="post-tail-wrapper text-muted"><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=《动手学深度学习》笔记 - Eternity-AIBN&u=https://eternity-aibn.github.io/posts/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=《动手学深度学习》笔记 - Eternity-AIBN&url=https://eternity-aibn.github.io/posts/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Pytorch%E5%AD%A6%E4%B9%A0/">Pytorch学习</a></li><li><a href="/posts/Leetcode%E9%9A%8F%E7%BC%98%E5%88%B7%E9%A2%98/">Leetcode随缘刷题</a></li><li><a href="/posts/PyQt%E5%AD%A6%E4%B9%A0/">Pyqt学习</a></li></ul></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/PyQt%E5%AD%A6%E4%B9%A0/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Feb 9, 2021 <i class="unloaded">2021-02-09T00:00:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Pyqt学习</h3><div class="text-muted small"><p> 参考教程 一、第一个例子 一个应用的组件是分层结构，其中大多组件都有父类，没有父类的组件是顶级窗口 简单例子 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import sys from PyQt5.QtWidgets import QApplication,QWidget if __name__ == '__main__': app = QA...</p></div></div></a></div><div class="card"> <a href="/posts/Pytorch%E5%AD%A6%E4%B9%A0/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 20, 2020 <i class="unloaded">2020-09-20T00:00:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Pytorch学习</h3><div class="text-muted small"><p> 一、比基础还基础的基础 1. 一些基本操作 构建张量：torch.Tensor(5,3) 零矩阵：torch.zeros(5,3,dtype=torch.long) 在已有张量基础上：x=x.new_ones(5,3) 获取张量大小：x.size() 原地操作：_后缀，如y.add_(x) ...</p></div></div></a></div><div class="card"> <a href="/posts/Leetcode%E9%9A%8F%E7%BC%98%E5%88%B7%E9%A2%98/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 16, 2020 <i class="unloaded">2020-09-16T00:00:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Leetcode随缘刷题</h3><div class="text-muted small"><p> 239. 滑动窗口最大值 2020-9-16 题目描述 给定一个数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。返回滑动窗口中的最大值。 进阶：你能在线时间复杂度内解决此题吗？ 示例： 输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3 输出: [3,...</p></div></div></a></div></div></div><!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Hello-World/" class="btn btn-outline-primary"><p>Hello World</p></a> <a href="/posts/Leetcode%E9%9A%8F%E7%BC%98%E5%88%B7%E9%A2%98/" class="btn btn-outline-primary"><p>Leetcode随缘刷题</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/Eternity-AIBN">Augenstern</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://eternity-aibn.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
