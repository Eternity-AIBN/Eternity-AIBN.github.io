### 一、比基础还基础的基础

##### 1. 一些基本操作

- 构建张量：__torch.Tensor(5,3)__

- 零矩阵：__torch.zeros(5,3,dtype=torch.long)__

- 在已有张量基础上：__x=x.new_ones(5,3)__

- 获取张量大小：__x.size()__

- 原地操作：_____后缀，如__y.add_(x)__

- 调整大小：__torch.view__

- 单元素张量可用__.item()__获取其值

- [-->更多张量操作](https://pytorch.org/docs/stable/torch.html)



##### 2. 张量与numpy数组的转换

- 张量→numpy数组：__b=a.numpy()__

* numpy数组→张量：__b=torch.from_numpy(a)__





### 二、Autograd自动求导

- 神经网络的核心：__autograd__包

![image-20200923151757917](C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200923151757917.png)

- 通过__.requires_grad()__可更改现有Tensor的__requires_grad__标志





### 三、神经网络

##### 1. 典型训练过程

- 定义神经网络模型,它有一些可学习的参数(或者权重);
- 在数据集上迭代;
- 通过神经网络处理输入;
- 计算损失(输出结果和正确值的差距大小)
- 将梯度反向传播会网络的参数;
- 更新网络的参数,主要使用如下简单的更新原则:`weight = weight - learning_rate * gradient`

##### 2. 定义网络

- __view函数__将矩阵重新排列（改变维度）

- 只需定义__forward__函数，__backward__函数会自动创建

- 注意：__torch.nn__ 只支持小批量输入，而不支持单个样本——即网络层将接受一个四维张量（`𝑛𝑆𝑎𝑚𝑝𝑙𝑒𝑠×𝑛𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝑠×𝐻𝑒𝑖𝑔ℎ𝑡×𝑊𝑖𝑑𝑡ℎ`），若只有单个样本，可使用__input.unsqueeze(0)__增加维数
- 反向传播时需要**清除已存在的梯度**（__zero_grad()__），否则梯度将被累加到其中
- 















