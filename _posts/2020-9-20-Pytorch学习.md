### 一、比基础还基础的基础

##### 1. 一些基本操作

- 构建张量：==torch.Tensor(5,3)==

- 零矩阵：==torch.zeros(5,3,dtype=torch.long)==

- 在已有张量基础上：==x=x.new_ones(5,3)==

- 获取张量大小：==x.size()==

- 原地操作：==_==后缀，如==y.add_(x)==

- 调整大小：==torch.view==

- 单元素张量可用==.item()==获取其值

- [-->更多张量操作](https://pytorch.org/docs/stable/torch.html)



##### 2. 张量与numpy数组的转换

- 张量→numpy数组：==b=a.numpy()==

* numpy数组→张量：==b=torch.from_numpy(a)==





### 二、Autograd自动求导

- 神经网络的核心：==autograd==包

![image-20200923151757917](C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200923151757917.png)

- 通过==.requires_grad()==可更改现有Tensor的==requires_grad==标志





### 三、神经网络

##### 1. 典型训练过程

- 定义神经网络模型,它有一些可学习的参数(或者权重);
- 在数据集上迭代;
- 通过神经网络处理输入;
- 计算损失(输出结果和正确值的差距大小)
- 将梯度反向传播会网络的参数;
- 更新网络的参数,主要使用如下简单的更新原则:`weight = weight - learning_rate * gradient`

##### 2. 定义网络

- ==view函数==将矩阵重新排列（改变维度）

- 只需定义==forward==函数，==backward==函数会自动创建

- 注意：==torch.nn== 只支持小批量输入，而不支持单个样本——即网络层将接受一个四维张量（`𝑛𝑆𝑎𝑚𝑝𝑙𝑒𝑠×𝑛𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝑠×𝐻𝑒𝑖𝑔ℎ𝑡×𝑊𝑖𝑑𝑡ℎ`），若只有单个样本，可使用==input.unsqueeze(0)==增加维数
- 反向传播时需要**清除已存在的梯度**（==zero_grad()==），否则梯度将被累加到其中
- 















