---
title: Pytorch学习
date: 2020-09-20 00:00:00 +0800
categories: [Machine learning]
tags: [deep learning, pytorch]     # TAG names should always be lowercase
---

### 一、比基础还基础的基础

##### 1. 一些基本操作

- 构建张量：`torch.Tensor(5,3)`

- 零矩阵：`torch.zeros(5,3,dtype=torch.long)`

- 在已有张量基础上：`x=x.new_ones(5,3)`

- 获取张量大小：`x.size()`

- 原地操作：`_`后缀，如`y.add_(x)`

- 调整大小：`torch.view`

- 单元素张量可用`.item()`获取其值

- [-->更多张量操作](https://pytorch.org/docs/stable/torch.html)



##### 2. 张量与numpy数组的转换

- 张量→numpy数组：`b=a.numpy()`

* numpy数组→张量：`b=torch.from_numpy(a)`





### 二、自动微分

- 神经网络的核心：`autograd`包

- 通过`.requires_grad()`可更改现有Tensor的`requires_grad`标志（跟踪tensor的所有操作）

- 调用`.backward()`自动计算所有梯度，张量的梯度将累积到`.grad`属性中

- 停止对tensor的跟踪：

    - 调用`.detach()`
    - 将代码块用`with torch.no_grad()`包装

- `Function`类：Tensor和Function构建一个__非循环图__，保存整个完整计算过程的历史信息；张量的`.grad_fn`属性保存着创建张量的__Function的引用__（自己创建张量则该值为None）

- gradient参数指定张量形状？？

    





### 三、神经网络

##### 1. 典型训练过程

- 定义神经网络模型,它有一些可学习的参数(或者权重)
- 在数据集上迭代
- 通过神经网络处理输入
- 计算损失(输出结果和正确值的差距大小)
- 将梯度反向传播会网络的参数
- 更新网络的参数

##### 2. 定义网络

* 通过`torch.nn`构建，`nn.Module`包括层和方法`forawrd(input)`，返回输出`output	`

- 只需定义`forward`函数，`backward`函数会自动创建
- `net.parameters()`可获得一个模型可训练的参数
- 注意：`torch.nn`只支持小批量输入，而不支持单个样本——即网络层将接受一个四维张量（`𝑛𝑆𝑎𝑚𝑝𝑙𝑒𝑠×𝑛𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝑠×𝐻𝑒𝑖𝑔ℎ𝑡×𝑊𝑖𝑑𝑡ℎ`），若只有单个样本，可使用`input.unsqueeze(0)`增加维数
- 反向传播时需要**清除已存在的梯度**（`zero_grad()`），否则梯度将被累加到其中

##### 3、更新参数

* 随机梯度下降：`weight = weight - learning_rate * gradient`
* `torch.optim`中有其他更新规则（SGD、Adam等）







### 四、数据加载和处理

* `torch.utils.data.Dataset`：数据集抽象类，自定义数据集应继承该类并覆盖以下方法
    * `__len__`：返回数据集尺寸
    * `__getitem__`：获取索引数据

* `torch.utils.data.DataLoader`：数据加载器，提供常用操作
    * `batch_size`：每个batch的大小
    * `shuffle`：是否进行shuffle操作
    * `num_workers`：加载数据时使用几个子进程









