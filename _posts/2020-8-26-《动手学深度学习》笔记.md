## 一、MXNet 基础

1. `asscalar`函数将向量转换为标量

2. 广播机制

3. NDArray 与 Numpy 转换：

   ```python
   D = nd.array(P)  # Numpy->NDArray
   D.asnumpy() 	# NDArray->Numpy
   ```

4. 自动求梯度：

   ```python
   x.attach_grad()		# 申请存储梯度所需要的内存
   with autograd.record():	# 要求MXNet记录与求梯度有关的计算
       y = f(x)
   y.backward()		# 自动求梯度
   ```

5. 以线性回归模型为例：

   ```python
   #读取数据集
   from mxnet.gluon import data as gdata
   dataset = gdata.ArrayDataset(features,lables) # 将特征和标签组合
   data_iter = gdata.DataLoader(dataset,batch_size,shuffle=True) #随机读取小批量
   
   #定义模型
   from mxnet.gluon import nn
   net = nn.Sequential()  #模型变量，串联各层的容器，构造模型时在该层依次添加层
   net.add(nn.Dense(1))   #Dense实例——全连接层，无需指定输入形状，模型会自动推断
   
   #初始化模型参数
   from mxnet import init
   net.initialize(init.Normal(sigma=0.01)) #指定权重参数；偏差参数默认初始化为0
   
   #定义损失函数
   from mxnet.gluon import loss as gloss
   loss = gloss.L2Loss()  	#平方损失（L2范数损失）
   
   #定义优化算法
   from mxnet import gluon
   trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.03})  	#sgd小批量随机梯度下降
   
   #训练模型
   num_epochs = 3
   for epoch in range(1, num_ephchs + 1):
       for X, y in data_iter:
           with autofrid.record():
               l = loss(net(X), y)  	#计算loss
           l.backward()			  #反向传播
           trainer.step(batch_size) 	#更新模型参数
       l = loss(net(feature), lables) 	#计算全部loss
       print('epoch %d, loss:%f'%(epoch, l.mean().asnumpy()))
   ```

6. 计算图：可视化运算符和变量在计算中的依赖关系
   方框代表变量，圆圈代表运算符，箭头表示从输入到输出之间的依赖关系

7. Xavier随机初始化：假设某全连接层输入个数为 a，输出个数为 b，该层中权重参数的每个元素**随机采样**于**均匀分布**：$U(-\sqrt \frac{6}{a+b},\sqrt \frac{6}{a+b})$
   主要考虑到：每层输出的方差不应该受该层输入个数影响，每层梯度的方差也不改受该层输出个数影响



## 二、深度学习计算

### 1. 模型构造

#### 1.使用Sequential类

#### 2.基于Block类的构造方法

Block类是一个可供自由组建的部件，其子类既可以是**一个层（Dense）**，也可是**一个模型**，或者是**模型的一部分**

eg. 继承Block类实现MLP：

```python
from mxnet import nd
from mxnet.gluon import nn

class MLP(nn.Block):
    # 声明带有模型参数的层，这里声明了两个全连接层
    def __init__(self, **kwargs):
        # 调用父类Block的构造函数进行必要的初始化，这样在构造实例时还可以指定其他函数
        # 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params
        super(MLP, self).__init__(**kwargs)
        self.hidden = nn.Dense(256, activation='relu')  # 隐藏层
        self.output = nn.Dense(10)  # 输出层

    # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出
    def forward(self, x):
        return self.output(self.hidden(x))
```

（上述MLP类无需定义反向传播函数——系统将通过自动求梯度而自动生成backward函数）

### 2. 模型参数访问、初始化、共享

#### 1.访问

对于Sequential构造的神经网络：

```python
# 访问第一层（[0]）包含的所有参数（params）
net[0].params

# 访问特定参数 （Gluon中参数类型为Parameter类）
net[0].params['dense0_weight'] #通过名字访问字典里的元素
net[0].weight				   #直接使用其变量名

# 访问上诉Parameter类中的参数和梯度数值
net[0].weight.data()
net[0].weight.grad()

# 获取net变量所有嵌套的层包含的参数
net.collect_params()
# 利用正则表达式匹配参数名
net.collect_params('.*weight')
```

#### 2. 初始化

```python
# 非首次对模型初始化需要指定force_reinit为真
# 正态分布数
net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)
# 常数
net.initialize(init=init.Constant(1), force_reinit=True)

# 对某个特定参数进行初始化——调用Parameter类的initialize函数
net[0].weight.initialize(init=init.Xavier(), force_reinit=True)

# set_data直接改写模型参数
net[0].weight.set_data(net[0].weight.data() + 1)
```

自定义初始化：实现一个`Initializer类`的子类，通常只需要实现`_init_weight`函数，将传入的NDArray修改成初始化的结果

```python
class MyInit(init.Initializer):
    def _init_weight(self, name, data):
        ...
net.initialize(MyInit(), force_reinit=True)
```

#### 3. 共享（多个层之间）

1. 重复调用同一个层

2. 构造层时指定使用特定参数（使用同一份参数）

   ```python
   net = nn.Sequential()
   shared = nn.Dense(8, activation='relu')
   net.add(shared,
           nn.Dense(8, activation='relu', params=shared.params),
           nn.Dense(10))
   net.initialize()
   ```

### 3.模型参数延后初始化

1. 使用层时未指定输入个数——只有当将确定形状的输入`X`传进网络中进行前向计算`net(X)`时，系统才推断出该层的权重参数形状，此时才能真正开始初始化参数。
2. 好处：模型更加简单，只需定义每个层的输出大小
   坏处：第一次前向计算之前无法直接操作模型参数（如无法用data函数和set_data函数获取和修改参数）——通常会额外做一次前向计算迫使参数被真正初始化
3. 如何避免：指定输入个数（使用`in_units`）
   另，对已初始化的模型重新初始化时也不会发生延后初始化

### 4.读取、存储

1. 直接使用`save`函数和`load`函数存储和读取NDArray

   ```python
   nd.save('x',x)	#将NDArray变量X存在名为x的文件中
   x2=nd.load('x')
   ```

2. 读写Gluon模型参数：`save_parameters`和`load_parameter`函数



## 三、卷积神经网络CNN

### 1.二维卷积层

1. 互相关运算
2. 卷积层模型参数包括**卷积核**和**标量偏差**，通常先对卷积核随机初始化，然后不断迭代卷积核和偏差
3. 特征图：二维卷积层输出的二维数组，可以看作输入在空间维度（宽和高）上某一级的表征
   感受野：影响元素 x 的前向计算的所有可能输入区域

### 2.填充与步幅

1. 步幅：输入元素无法填满窗口时**无结果输出**
   <img src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824100823687.png" alt="image-20200824100823687" style="zoom:67%;" />
2. 默认情况填充为 0 ，步幅为 1

### 3.多输入/输出通道

1. 第三维：通道维
2. 输入数据含多个通道时，需要一个输入通道数与输入数据**通道数相同**的卷积核（**逐通道计算，按通道相加**）<img src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824101154603.png" alt="image-20200824101154603" style="zoom: 67%;" />
3. 由上，输出通道数总为1；
   想得到多通道输出：每个输出通道使用一个核数组<img src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824102721253.png" alt="image-20200824102721253" style="zoom: 50%;" />

### 4.池化层

1. 缓解卷积层对位置的过度敏感性

2. 指定非正方形的池化窗口、填充和步幅

   ```python
   nn.MaxPool2D((2, 3), padding=(1, 2), strides=(2, 3))
   ```

3. 处理多通道数据时**对每个通道分别池化**——输出通道数与输入通道数相等



### 5.卷积神经网络 LeNet

![image-20200824165321810](C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824165321810.png)

1. 卷积层块 + 全连接层块
2. 卷积层块：（卷积层+最大池化层）× 2
   卷积层识别图像空间模式，最大池化层降低卷积层对位置敏感性
   卷积层块输出形状：批量大小、通道、高、宽
   全连接层会将小批量中每个样本变平（即输入形状变成二维）——批量大小、通道 * 高 * 宽



### 6.深度卷积神经网络

![image-20200824201439617](C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200824201439617.png)

与LeNet区别：
① 包含**8层变换**——5层卷积和2层全连接隐藏层、1个全连接输出层
② **ReLU**激活函数（更简单，消除**梯度消失**）
③ 使用 **dropout** 控制全连接层的模型复杂度
④ 引入**图像增广**，扩大数据集以缓解过拟合



### 7.使用重复元素的网络 VGG

1. 重复使用简单的基础块——连续使用数个相同的填充为1、窗口形状 3×3 的卷积层后接上一个步幅为2、窗口形状 2×2 的最大池化层
2. 卷积层保持输入的高和宽不变，池化层对其减半
3. VGG-11：8个卷积层（单卷积层 * 2+双卷积层 * 3）+3个全连接层
4. 高、宽减半，通道翻倍



### 8.网络中的网络 NiN

1. 上述三个网络：先卷积层抽取空间特征，再以全连接层输出分类结果
   NiN：**串联**多个由卷积层和全连接层（实际上是 1×1 卷积层）构成的小网络
2. NiN块：一个卷积层 + 两个1×1卷积层（前者超参数自行设置，后者一般固定）
3. NiN模型：每个NiN块后接一个最大池化层
   NiN 去掉了AlexNet最后的3个全连接层，使用**输出通道数等于标签类别数的NiN块**，然后用**全局平均池化层**对每个通道中所有元素**求平均**并直接用于分类——显著减小模型参数量、缓解过拟合
   <img src="C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200826200240356.png" alt="image-20200826200240356" style="zoom:50%;" />



### 9.含并行连结的网络 GoogLeNet

1. Inception块
   ![image-20200826204310913](C:\Users\15542\AppData\Roaming\Typora\typora-user-images\image-20200826204310913.png)
   并行线路，都采用了合适的填充使输入输出高宽一致；可自定义的超参数为每个层中的**输出通道数**
2. [GoogLeNet模型](https://static.oschina.net/uploads/space/2018/0317/141544_FfKB_876354.jpg)含 5 个模块，模块间使用最大池化层减小输出高宽
   ① 64 通道的7×7卷积层
   ② 1×1卷积 + 3×3卷积（对应第二条线路）
   ③ 串联2个完整的Inception块
   ④ 串联5个Inception块
   ⑤ 2个Inception块（后接全局平均化层、全连接层）



### 10.批量归一化

1. 标准化处理输入数据使**各个特征的分布相近**，**易于训练出有效的模型**

   训练中模型参数的更新容易造成**靠近输出层**输出的**剧烈变化**——数值的**不稳定性**，导致难以训练出有效模型

   批量归一化**不断调整神经网络中间输出**

2. 对全连接层：置于仿射变化和激活函数之间
   先对小批量求均值$μ_B$ 和方差$\sigma _B^2$ ，然后标准化（按元素开方和除法）$x^{(i)}_{hat}=\frac{x^{(i)}-μ_B}{\sqrt{\sigma^2_B+\epsilon}}$
   引入拉伸参数$\gamma$和偏移参数$\beta$，按元素乘法：$y^{(i)}=\gamma ⊙x^{(i)}_{hat}+\beta$

3. 对卷积层：卷积计算后、激活函数前。含多通道时各通道单独进行批量归一化

4. 训练时可将批量设置**大一点**，使均值和方差计算较为准确；
   预测时使用**移动平均**估算均值和方差.



### 11.残差网络 ResNet

1. 残差块：输入可通过跨层的数据线路更快向前传播